'\"
'\" tdbc_pool.n --
'\"
'\" Copyright (c) 2020 by Cyan Ogilvie.
'\"
'\" See the file "license.terms" for information on usage and redistribution of
'\" this file, and for a DISCLAIMER OF ALL WARRANTIES.
'\"
.TH "tdbc::pool" n 8.6 Tcl "Tcl Database Connectivity"
'\" .so man.macros
'\" IGNORE
.if t .wh -1.3i ^B
.nr ^l \n(.l
.ad b
'\"	# BS - start boxed text
'\"	# ^y = starting y location
'\"	# ^b = 1
.de BS
.br
.mk ^y
.nr ^b 1u
.if n .nf
.if n .ti 0
.if n \l'\\n(.lu\(ul'
.if n .fi
..
'\"	# BE - end boxed text (draw box now)
.de BE
.nf
.ti 0
.mk ^t
.ie n \l'\\n(^lu\(ul'
.el \{\
'\"	Draw four-sided box normally, but don't draw top of
'\"	box if the box started on an earlier page.
.ie !\\n(^b-1 \{\
\h'-1.5n'\L'|\\n(^yu-1v'\l'\\n(^lu+3n\(ul'\L'\\n(^tu+1v-\\n(^yu'\l'|0u-1.5n\(ul'
.\}
.el \}\
\h'-1.5n'\L'|\\n(^yu-1v'\h'\\n(^lu+3n'\L'\\n(^tu+1v-\\n(^yu'\l'|0u-1.5n\(ul'
.\}
.\}
.fi
.br
.nr ^b 0
..
'\"	# CS - begin code excerpt
.de CS
.RS
.nf
.ta .25i .5i .75i 1i
..
'\"	# CE - end code excerpt
.de CE
.fi
.RE
..
'\" END IGNORE
.BS
.SH "NAME"
tdbc::pool \- TDBC Connect Pooling
.SH "SYNOPSIS"
.nf
package require \fBtdbc 1.0\fR

\fBtdbc::pool\fR \fIdriver\fR \fBcreate \fIdb\fR ?\fI-option value\fR...?
\fBtdbc::pool\fR \fIdriver\fR \fBnew\fR ?\fI-option value\fR...?
\fIdb\fB forceclose\fR
\fBtdbc::poolDestroy\fR \fIdriver\fR ?\fI-option value\fR...?
.fi
.BE
.SH "DESCRIPTION"
.PP
There is often a significant latency cost when creating new database
connections (at least 5 - 20 milliseconds even under ideal conditions),
and so it is desirable to reuse existing idle connections for
applications like web servers where the individual users of a database
handle are short lived (often on the tens of milliseconds timescale
themselves).  A common approach to this is to provide a pool of open
database connections from which handles are issued to workers and to
which they return when the worker is finished with them.  \fBtdbc::pool\fR
provides such a facility for connections created by tdbc drivers.
.PP
To use the connection pooling facility in an application, replace the
normal call to the driver connection constructor:

.CS
\fBtdbc::\fIdriver\fB::connection create \fIdb\fR ?\fI-option value\fR...?
.CE

with

.CS
\fBtdbc::pool \fIdriver\fB create \fIdb\fR ?\fI-option value\fR...?
.CE

or similar using the \fBnew\fR constructor method.  When the handle issued
by \fB::tdbc::pool\fR is destroyed, either explicitly through the 
\fBdestroy\fR or \fBclose\fR methods, is renamed to an empty string,
or anything equivalent to these (like having its containing namespace
deleted), it is released back to the pool.
.PP
If the application needs to override the behaviour of automatically
returning closed handles back to the pool, it can call \fBforceclose\fR
on the handle instead, which will close the connection to the database
and destroy the connection instance, exactly as usually happens when
non-pooled handles are closed.
.PP
The \fBtdbc::poolDestroy\fR command allows an application to close all
detached connections waiting in the given pool and release any resources
used to manage the pool, but most applications won't need to do that.
.SH "POOL MANAGEMENT"
.PP
Pools are implicit when using \fBtdbc::pool\fR, being defined as the
the combination of the \fIdriver\fR and the (ordered) set of options
passed to the connection constructor.  All requests to \fBtdbc::pool\fR
with exactly the same set of \fIdriver\fR and \fI-option value\fR...
pairs are considered to be requests for connections from the same pool.
Reordering the options or any other change names a different pool.
.PP
Part of the objective of connection pooling is to hide the latency of
establishing new connections to the database, and so \fBtdbc::pool\fR
attempts to always have at least one idle handle available in the pool
to issue to the next requestor.  If a situation does arise in which the
pool is empty when a connection is requested, a new connection will
be created to fulfill the request.
.PP
Idle handles that are returned to the pool will be issued again in
the reverse order in which they were returned, that is
the last returned handle will be the first issued out again.  In this
way the pool behaves like a LIFO stack of handles.  The reason for
this is to minimize the set of handles in use, allowing handles in
excess of recent requirements to time out and be closed, freeing
connection resources on the database server.
.PP
Many database servers will close connections that are idle for more
than a few minutes, and so the handles sitting in the pool are 
periodically checked and removed from the pool if they are no longer
connected to the database.  A side-effect of this check with many
database servers is to flag the handle as not idle, preventing it from
being closed by the server.  Handles that have been in the pool
for roughly two minutes without being issued out again will be
closed and removed from the pool.  In this way the pool should
quickly scale up and down to meet the current application requirements
for the maximum (high water mark) number of handles required
simulaneously, plus one in reserve.
.SH "CONNECTION STATE"
.PP
If a handle is in an open transaction state when it returns to the
pool the transaction is rolled back, replicating the behaviour that
would occur if this handle was not being managed in a pool and was
destroyed in an open transaction state.  This also ensures that
connections issued from the pool are in a known transaction state
(no current transaction).

Other aspects of the connection state are not reset by being released to the
pool however, so state like session parameters (time zone, character set, etc),
connection-scoped temporary tables, session variables and so on are in whatever
state the last user of that handle left them, and are visible to the next user
of that handle.  Applications should take care that their use of pooled
database connections is robust to this hysteresis of connection state,
typically by avoiding modifying the state, or by explicitly setting any
modified state to known values when receiving a handle from the pool (although
this comes at a cost that somewhat offsets the gains made by using a connection
pool in the first place).
.SH "THREADS"
.PP
The situations in which connection pools are beneficial are also those
that tend to favour heavily threaded application architectures, and so
\fBtdbc::pool\fR fully supports multithreaded operation.  Handles are
issued to all requesting threads from a single shared pool, and handles
returned to the pool from a thread can be reissued to any other thread.
Threaded applications also often face difficulties in having to over-allocate
database handles to ensure that each thread has the maximum number of handles
it needs to do its work.  Using a connection pool addresses this by uncoupling
the number of database handles required from the number of threads, instead
allowing the number of allocated handles to follow the work load.
.PP
If the Thread package is not available in the interpreter when
\fBtdbc::pool\fR is called (perhaps because the build of Tcl does not support
threads), then the \fBtdbc::pool\fR command behaves like an alias of the
standard driver connection constructor.
.PP
A light-weight background thread is required by the pool management system
to periodically check the status of detached handles in the pools.  It is
started by the first call to \fBtdbc::pool\fR, and stopped when the last
pool is destroyed by \fBtdbc::poolDestroy\fR.  While running, roughly
every twenty seconds the thread wakes up and tests every idle handle in every
pool for connectivity with the database server and removes any that are stale
or have been idle for longer than two minutes.
.SH "DRIVER SUPPORT"
.PP
The \fBtdbc::pool\fR command will issue handles for any tdbc driver,
whether the driver supports the underlying features required for connection
pooling or not.  If the driver lacks support, \fBtdbc::pool\fR falls
back to creating a new handle for each request, and when that handle is
closed it is simply destroyed, making \fBtdbc::pool\fR behave like an
alias for the standard driver connection constructor.  In this way
applications can safely use \fBtdbc::pool\fR in place of the standard
driver constructors without needing to know if a particular driver supports
connection pooling, but it will obviously only see a latency benefit
when using drivers that support connection pooling.
.PP
Drivers wishing to support connection pooling need to provide a \fBdetach\fR
method which unlinks the underlying database connection from the connection
object and returns a handle that can be passed to a future call of the
connection constructor in the \fB-attach \fI$handle\fR option in order to
wrap the detached database connection in a new connection instance.
The \fBdetach\fR and later \fB-attach\fR may be in different threads, so
the state associated with the handle by the driver must be able to
move between Tcl interpreters and threads.  The semantics closely mirror
those of \fBthread::detach\fR and \fBthread::attach\fR for Tcl channels,
and drivers implemented in Tcl script using socket channels to communicate
with a database server could implement this interface using trivial wrappers
around \fBthread::detach\fR and \fBthread::attach\fR.
.PP
See the documentation for tdbc::connection for the details of the requirements
for \fBdetach\fR and \fB-attach\fR.
.PP
A driver may also provide a more efficient implementation of the
\fBconnected\fR method than the default of attempting a "\fBselect 1\fR"
if such exists for the database it implements.
.PP
Drivers can also provide internal optimizations for efficiently preserving
and restoring prepared statements associated with a handle that is detached
and later reattached to.  This optimization is invisible to the Tcl script
layer other than dramatically reducing the cost for recreating prepared
statements that had previously been associated with that handle.
.PP
Currently the \fBpostgres\fR and \fBmysql\fR drivers have full support for
connection pooling and optimized prepared statement caching.
.SH "EXAMPLE"
.PP
.CS
 oo::class create customer {
     variable cust_id

     constructor {id} {
         set cust_id $id
         package require tdbc::postgres
         \fBtdbc::pool postgres create db -host localhost -db example\fR
     }

     method getEmail {} {
         lindex [\fBdb\fR allrows -as lists {
             select
                 email
             from
                 customers
             where
                 id = :cust_id
         }] 0 0
     }

     method setEmail {newemail} {
         \fBdb\fR allrows {
             update
                 customers
             set
                 email = :newemail
             where
                 id = :cust_id
         }
     }
 }

 foreach id {1234 4321 4242} {
     set cust    [customer new $id]
     # A database handle is issued from the pool

     puts "current email for $id: [$cust getEmail]"
     $cust setEmail updated@customer.com
     $cust destroy

     # The database connection is released back to the pool because the
     # $cust instance namespace is deleted, triggering the db command
     # to be deleted, which returns the handle to the pool.
     #
     # Each iteration through this loop with just reuse the existing
     # database connection, so performance will be good.
     #
     # Additionally, since the postgres driver supports prepared
     # statement caching across detach / attach, the statements used
     # by the getEmail and setEmail methods will only be prepared
     # once, and will be reused for each subsequent execution, greatly
     # reducing execution overhead.
 }
.CE
.PP
If the situation warranted it, this loop could dispatch its work to a tpool(n)
of workers which perform the loop iterations in parallel, all sharing a set of
database handles and still benefiting from the prepared statement caching.
Whether this is a sensible thing to do would depend on the CPU time taken for
each loop body vs the time to collect the results, and the relative CPU load
for the work done on the client vs the work done on the database server, but
it will address the dead time on each side of the database connection as
requests and responses ping-ping back and forth serially:

.CS
 package require Thread

 proc threadedLmap {args} {
     set tpool [tpool::create -maxworkers 16 -initcmd {
         oo::class create customer {
             # Class definition as above
         }
     }]

     try {
         set res {}
         set loopvars   {}
         foreach {vlist target} [lrange $args 0 end-1] {
             lappend loopvars {*}$vlist
         }
         set script     [lindex $args end]
         set waiting [lmap {*}[lrange $args 0 end-1] {
             tpool::post -nowait $tpool \\
                [list lassign [lmap v $loopvars {set $v}] {*}$loopvars]\\n$script
         }]

         while {[llength $waiting]} {
             foreach done [tpool::wait $tpool $waiting waiting] {
                 lappend res [tpool::get $tpool $done]
             }
         }

         set res
     } finally {
         tpool::release $tpool
     }
 }

 set results [threadedLmap id $very_long_list_of_ids {
     set cust    [customer new $id]
     set old     [$cust getEmail]
     $cust set_email cust$id@customer.com
     $cust destroy
     set old
 }]
.CE
.SH "SEE ALSO"
tdbc(n), tdbc::connection(n)
.SH "KEYWORDS"
TDBC, SQL, database, connection pool, thread
.SH "COPYRIGHT"
Copyright (c) 2020 by Cyan Ogilvie.
'\" Local Variables:
'\" mode: nroff
'\" End:
'\" vim: ft=nroff
